{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lighteval Example with Eval Hub SDK\n",
    "\n",
    "This notebook demonstrates how to use the EvalHub client SDK with the EvalHub evaluation service. This notebook uses `SyncEvalHubClient`.\n",
    "\n",
    "## Prerequisites: EvalHub credentials (`.env`)\n",
    "\n",
    "Before running this notebook, create a `.env` file with your EvalHub credentials. Run the block below in a terminal (e.g. from the `examples/` directory); it writes `EVALHUB_TOKEN` and `EVALHUB_URL` to `.env`. The notebook will use these when it loads the client (e.g. via `load_dotenv()`).\n",
    "\n",
    "```bash\n",
    "# Get your OpenShift authentication token\n",
    "export EVALHUB_TOKEN=$(oc whoami -t)\n",
    "\n",
    "# Get the EvalHub route URL from your namespace (e.g., 'test')\n",
    "\n",
    "export EVALHUB_URL=https://$(oc get route evalhub -n test -o jsonpath='{.spec.host}')\n",
    "\n",
    "# Write to .env file\n",
    "cat > .env << EOF\n",
    "EVALHUB_TOKEN=$EVALHUB_TOKEN\n",
    "EVALHUB_URL=$EVALHUB_URL\n",
    "EOF\n",
    "\n",
    "# Verify\n",
    "echo \"Token: ${EVALHUB_TOKEN:0:20}...\"\n",
    "echo \"URL: $EVALHUB_URL\"\n",
    "echo \"Written to .env\"\n",
    "```\n",
    "\n",
    "\n",
    "## OpenShift Authentication\n",
    "\n",
    "The SDK has built-in support for OpenShift authentication with three methods:\n",
    "\n",
    "1. **ServiceAccount Token** (`auth_token_path`): For pods running inside OpenShift, use the automounted token at `/var/run/secrets/kubernetes.io/serviceaccount/token`\n",
    "2. **User Token** (`auth_token`): For local development, use token from `oc whoami -t` (shown above)\n",
    "3. **Environment Variable** (`auth_token`): For production, use token from environment variable or secret (used in examples below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read environment variables from .env if present (for local runs)\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evalhub import (\n",
    "    SyncEvalHubClient,\n",
    "    JobSubmissionRequest,\n",
    "    BenchmarkConfig,\n",
    "    ModelConfig,\n",
    "    JobStatus,\n",
    ")\n",
    "import os\n",
    "\n",
    "# Verify environment variables are set\n",
    "if not os.getenv(\"EVALHUB_TOKEN\"):\n",
    "    raise ValueError(\n",
    "        \"EVALHUB_TOKEN environment variable is not set. See Prerequisites section above.\"\n",
    "    )\n",
    "if not os.getenv(\"EVALHUB_URL\"):\n",
    "    raise ValueError(\n",
    "        \"EVALHUB_URL environment variable is not set. See Prerequisites section above.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to EvalHub and check health status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TLS verification disabled - skipping CA bundle detection\n",
      "TLS verification disabled (insecure mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ EvalHub is healthy: healthy\n",
      "  Version: unknown\n",
      "  Uptime: 0.0s\n"
     ]
    }
   ],
   "source": [
    "client = SyncEvalHubClient(\n",
    "    base_url=os.getenv(\"EVALHUB_URL\"),\n",
    "    auth_token=os.getenv(\"EVALHUB_TOKEN\"),\n",
    "    insecure=True,\n",
    ")\n",
    "try:\n",
    "    # Check health\n",
    "    health = client.health()\n",
    "    print(f\"✓ EvalHub is healthy: {health['status']}\")\n",
    "    print(f\"  Version: {health.get('version', 'unknown')}\")\n",
    "    print(f\"  Uptime: {health.get('uptime_seconds', 0):.1f}s\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Failed to connect: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Providers and Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Found 5 providers:\n",
      "  - garak: Garak\n",
      "  - guidellm: GuideLLM\n",
      "  - lighteval: Lighteval\n",
      "  - lm_evaluation_harness: LM Evaluation Harness\n",
      "  - ragas: RAGAS\n"
     ]
    }
   ],
   "source": [
    "# List available providers using nested resource structure\n",
    "# List all providers\n",
    "providers = client.providers.list()\n",
    "print(f\"✓ Found {len(providers)} providers:\")\n",
    "for provider in providers:\n",
    "    print(f\"  - {provider.id}: {provider.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are using lighteval provider_id for this notebook\n",
    "List available benchmarks for the provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "provider_id = \"lighteval\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Found 23 benchmarks for lighteval:\n",
      "  - commonsense_reasoning: Commonsense Reasoning Suite\n",
      "  - scientific_reasoning: Scientific Reasoning Suite\n",
      "  - physical_commonsense: Physical Commonsense Suite\n",
      "  - truthfulness: Truthfulness Suite\n",
      "  - math: Math Suite\n",
      "  - knowledge: Knowledge Suite\n",
      "  - language_understanding: Language Understanding Suite\n",
      "  - hellaswag: HellaSwag\n",
      "  - winogrande: Winogrande\n",
      "  - openbookqa: OpenBookQA\n",
      "  - arc:easy: ARC Easy\n",
      "  - arc:challenge: ARC Challenge\n",
      "  - piqa: PIQA\n",
      "  - truthfulqa:mc: TruthfulQA MC\n",
      "  - truthfulqa:generation: TruthfulQA Generation\n",
      "  - gsm8k: GSM8K\n",
      "  - math:algebra: MATH Algebra\n",
      "  - math:counting_and_probability: MATH Counting & Probability\n",
      "  - mmlu: MMLU\n",
      "  - triviaqa: TriviaQA\n",
      "  - glue:cola: GLUE CoLA\n",
      "  - glue:sst2: GLUE SST-2\n",
      "  - glue:mrpc: GLUE MRPC\n"
     ]
    }
   ],
   "source": [
    "benchmarks = client.benchmarks.list(provider_id=provider_id)\n",
    "print(f\"\\n✓ Found {len(benchmarks)} benchmarks for {provider_id}:\")\n",
    "for benchmark in benchmarks:\n",
    "    print(f\"  - {benchmark.id}: {benchmark.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit an Evaluation Job\n",
    "\n",
    "To find available models in your namespace:\n",
    "\n",
    "```bash\n",
    "# List services in your namespace\n",
    "oc get svc -n <namespace>\n",
    "# Example: Using a service named 'granite-llm-metrics' running in the 'prabhu' namespace on port 8080.\n",
    "# The model endpoint URL is formatted as:\n",
    "#   http://<service-name>.<namespace>.svc.cluster.local:<port>/v1\n",
    "# In this case, the endpoint points to the `granite-llm` model.\n",
    "#\n",
    "# Correct URL format: http://granite-llm-metrics.prabhu.svc.cluster.local:8080/v1\n",
    "# WRONG format: http://granite-llm-metrics.prabhu.svc.cluster.local:8080/v1/completions\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_url = \"http://granite-llm-metrics.prabhu.svc.cluster.local:8080/v1\"\n",
    "model_name = \"granite-llm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Job submitted successfully\n",
      "  Job ID: c02c0831-0097-4598-b4cb-5cdb0870904c\n",
      "  State: JobStatus.PENDING\n",
      "  Created: 2026-02-13 06:42:54.696571+00:00\n"
     ]
    }
   ],
   "source": [
    "# Create benchmark configuration\n",
    "benchmark1 = BenchmarkConfig(\n",
    "    id=\"gsm8k\",\n",
    "    provider_id=\"lighteval\",\n",
    "    parameters={\n",
    "        \"num_examples\": 5,  # Number of examples to evaluate (use small number for testing)\n",
    "        \"tokenizer\": \"google/flan-t5-small\",  # HuggingFace tokenizer for the model\n",
    "    },\n",
    ")\n",
    "\n",
    "# Create job submission request\n",
    "job_request = JobSubmissionRequest(\n",
    "    model=ModelConfig(url=model_url, name=model_name),\n",
    "    benchmarks=[benchmark1],\n",
    "    timeout_minutes=15,\n",
    "    retry_attempts=1,\n",
    ")\n",
    "\n",
    "# Submit job using nested resource\n",
    "job = client.jobs.submit(job_request)\n",
    "\n",
    "# Store job ID for later use\n",
    "submitted_job_id = job.id\n",
    "print(f\"✓ Job submitted successfully\")\n",
    "print(f\"  Job ID: {submitted_job_id}\")\n",
    "print(f\"  State: {job.state}\")\n",
    "print(f\"  Created: {job.resource.created_at}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Current job state: JobStatus.RUNNING\n",
      "  Message: Evaluation job is running\n"
     ]
    }
   ],
   "source": [
    "# Check status\n",
    "updated_job = client.jobs.get(submitted_job_id)\n",
    "print(f\"\\n✓ Current job state: {updated_job.state}\")\n",
    "if updated_job.status and updated_job.status.message:\n",
    "    print(f\"  Message: {updated_job.status.message.message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wait for the job to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Current job state: JobStatus.RUNNING\n",
      "\n",
      "✓ Current job state: JobStatus.RUNNING\n",
      "✓ Job completed successfully\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "while updated_job.state not in (JobStatus.COMPLETED, JobStatus.FAILED):\n",
    "    print(f\"\\n✓ Current job state: {updated_job.state}\")\n",
    "    time.sleep(20)\n",
    "    updated_job = client.jobs.get(submitted_job_id)\n",
    "\n",
    "if updated_job.state == JobStatus.FAILED:\n",
    "    if updated_job.status and updated_job.status.message:\n",
    "        print(f\"Job failed: {updated_job.status.message.message}\")\n",
    "    else:\n",
    "        print(\"Job failed: Unknown\")\n",
    "else:\n",
    "    print(f\"✓ Job completed successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "benchmark metrics: {'all.extractive_match': 0.8, 'gsm8k|0.extractive_match': 0.8}\n"
     ]
    }
   ],
   "source": [
    "print(\"benchmark metrics:\", updated_job.results.benchmarks[0].metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"resource\": {\n",
      "    \"id\": \"c02c0831-0097-4598-b4cb-5cdb0870904c\",\n",
      "    \"tenant\": \"TODO\",\n",
      "    \"created_at\": \"2026-02-13T06:42:54.696571Z\",\n",
      "    \"updated_at\": \"2026-02-13T06:43:24.914291Z\",\n",
      "    \"mlflow_experiment_id\": null,\n",
      "    \"message\": {\n",
      "      \"message\": \"Evaluation job is completed\",\n",
      "      \"message_code\": \"evaluation_job_updated\"\n",
      "    }\n",
      "  },\n",
      "  \"status\": {\n",
      "    \"state\": \"completed\",\n",
      "    \"message\": {\n",
      "      \"message\": \"Evaluation job is completed\",\n",
      "      \"message_code\": \"evaluation_job_updated\"\n",
      "    },\n",
      "    \"benchmarks\": [\n",
      "      {\n",
      "        \"id\": \"gsm8k\",\n",
      "        \"provider_id\": \"\",\n",
      "        \"status\": \"completed\",\n",
      "        \"message\": null\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"results\": {\n",
      "    \"benchmarks\": [\n",
      "      {\n",
      "        \"id\": \"gsm8k\",\n",
      "        \"provider_id\": \"\",\n",
      "        \"metrics\": {\n",
      "          \"all.extractive_match\": 0.8,\n",
      "          \"gsm8k|0.extractive_match\": 0.8\n",
      "        },\n",
      "        \"artifacts\": {\n",
      "          \"oci_digest\": \"sha256:0000000000000000000000000000000000000000000000000000000000000000\",\n",
      "          \"oci_reference\": \"localhost:5000/eval-results/gsm8k:c02c0831-0097-4598-b4cb-5cdb0870904c@sha256:0000000000000000000000000000000000000000000000000000000000000000\",\n",
      "          \"size_bytes\": 3072\n",
      "        },\n",
      "        \"mlflow_run_id\": null,\n",
      "        \"logs_path\": null\n",
      "      }\n",
      "    ],\n",
      "    \"mlflow_experiment_url\": null\n",
      "  },\n",
      "  \"model\": {\n",
      "    \"url\": \"http://granite-llm-metrics.prabhu.svc.cluster.local:8080/v1\",\n",
      "    \"name\": \"granite-llm\"\n",
      "  },\n",
      "  \"benchmarks\": [\n",
      "    {\n",
      "      \"id\": \"gsm8k\",\n",
      "      \"provider_id\": \"lighteval\",\n",
      "      \"parameters\": {\n",
      "        \"num_examples\": 5,\n",
      "        \"tokenizer\": \"google/flan-t5-small\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# print the job details\n",
    "print(updated_job.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the benchmark metrics from the job results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run this cell to delete the job with hard_delete=True\n",
    "client.jobs.cancel(submitted_job_id, hard_delete=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Close the client\n",
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
